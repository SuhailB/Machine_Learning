{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "my_data = pd.read_csv('data1.txt', sep='\\t')#,names=[\"f1\",\"f2\",\"f3\",\"f4\",\"f5\",\"f6\",\"f7\",\"f8\",\"f9\",\"f10\",\"f11\",\"f12\",\"f13\",\"f14\",\"f15\",\"l\"]) #read the data\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "#my_data = (my_data - my_data.mean())/my_data.std()\n",
    "\n",
    "#prepare X matrix\n",
    "X = my_data.iloc[:,0:2].values\n",
    "#ones = np.ones([X.shape[0],1])\n",
    "#X = np.concatenate((ones,X),axis=1) #X = pd.DataFrame.from_records(X)\n",
    "##prepare y matrix\n",
    "y = my_data.iloc[:,2:3].values #.values converts it from pandas.core.frame.DataFrame to numpy.ndarray\n",
    "##prepare w matrix\n",
    "w = np.zeros([1,2])\n",
    "\n",
    "b = 1\n",
    "\n",
    "C = 10\n",
    "alpha = 0.01\n",
    "epsilon = 0.04\n",
    "\n",
    "(X,y) =  make_blobs(n_samples=50,n_features=2,centers=2,cluster_std=1.05,random_state=40)\n",
    "y = np.where(y==0,-1,1)\n",
    "y = y.reshape(50,1)\n",
    "\n",
    "#plt.scatter(X[:,0],X[:,1], marker='o',c=y[:,0])\n",
    "#cost/loss function\n",
    "def Cost(X,y,w,b,C):\n",
    "    #regular format expression\n",
    "    sum1 = X @ w.T + b\n",
    "    sum1 = y * sum1\n",
    "    sum1 = 1-sum1  \n",
    "    sum1 = sum1.clip(min=0)\n",
    "    sum2 = np.power(w,2)\n",
    "    return (np.sum(sum2)/2) + C*np.sum(sum1)\n",
    "\n",
    "print(\"Cost\",Cost(X,y,w,b,10))\n",
    "\n",
    "def hinge_loss(w,x,y):\n",
    "    \"\"\" evaluates hinge loss and its gradient at w\n",
    "\n",
    "    rows of x are data points\n",
    "    y is a vector of labels\n",
    "    \"\"\"\n",
    "    loss,grad = 0,0\n",
    "    for (x_,y_) in zip(x,y):\n",
    "        v = y_*(np.dot(w,x_)+b)\n",
    "#        print(v)\n",
    "        loss += max(0,1-v)\n",
    "        grad += 0 if v > 1 else -y_*x_\n",
    "    return loss\n",
    "print(\"loss\",hinge_loss(w,X,y))\n",
    "\n",
    "def L_w(X,y,w,b,j):\n",
    "    z = X @ w.T + b\n",
    "    z = y*z\n",
    "    v = np.zeros([50,1])\n",
    "    Xj = X[:,j]\n",
    "    Xj = Xj.reshape(len(X),1)\n",
    "    z = np.where(z>=1,v,-1*y*Xj)\n",
    "    return z\n",
    " \n",
    "def L_b(y,w,b):\n",
    "    z = X @ w.T + b\n",
    "    z = y*z\n",
    "    v = np.zeros([50,1])\n",
    "    z = np.where(z>=1,v,-1*y)\n",
    "    return z\n",
    "\n",
    "def Batch_Gradient_w(X,y,w,b,j):\n",
    "    return w[0,j]+C*np.sum(L_w(X,y,w,b,j))\n",
    "\n",
    "def Batch_Gradient_b(y,w,b):\n",
    "    return b + C*np.sum(L_b(y,w,b))\n",
    "\n",
    "\n",
    "def Stocastic_Gradient_w(X,y,w,b,j,i):\n",
    "    Lw = L_w(X,y,w,b,j)\n",
    "    return w[0,j] + C*Lw[i,0]\n",
    "\n",
    "def Stocastic_Gradient_b(y,w,b,i):\n",
    "    Lb = L_b(y,w,b)\n",
    "    return b + C*Lb[i,0]\n",
    "\n",
    "def Batch_SVM(X,y,w,b,C,alpha,epsilon):\n",
    "    \n",
    "    k=0\n",
    "    cost = []\n",
    "    tempCost = 10\n",
    "    while(tempCost>epsilon):\n",
    "        \n",
    "        for j in range(len(w[0])):\n",
    "          w[0][j] = w[0][j] - alpha*Batch_Gradient_w(X,y,w,b,j)\n",
    "        b = b - alpha*Batch_Gradient_b(y,w,b)\n",
    "        \n",
    "        cost.append(Cost(X,y,w,b,C))\n",
    "        \n",
    "        if(k != 0):\n",
    "            tempCost = (abs(cost[k-1]-cost[k])*100)/cost[k-1]\n",
    "          \n",
    "        k += 1\n",
    "    return w,b,cost,k\n",
    "\n",
    "def Stocastic_SVM(X,y,w,b,C,alpha,epsilon):\n",
    "    \n",
    "    k=0\n",
    "    i=0\n",
    "    cost = []\n",
    "    tempCost = 10\n",
    "    m = len(X)\n",
    "    while(k<10000000):\n",
    "        \n",
    "        for j in range(len(w[0])):\n",
    "          w[0][j] = w[0][j] - alpha*Stocastic_Gradient_w(X,y,w,b,j,i)\n",
    "        b = b - alpha*Stocastic_Gradient_b(y,w,b,i)\n",
    "        \n",
    "        cost.append(Cost(X,y,w,b,C))\n",
    "        \n",
    "        if(k != 0):\n",
    "            tempCost = (abs(cost[k-1]-cost[k])*100)/cost[k-1]\n",
    "        i = (i+1)%m\n",
    "        k += 1\n",
    "    return w,b,cost,k\n",
    "\n",
    "w_,b_,c,count = Batch_SVM(X,y,w,b,C,alpha,epsilon)\n",
    "print(Cost(X,y,w_,b_,C))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "my_data = pd.read_csv('data.txt', sep='\\t')#,names=[\"f1\",\"f2\",\"f3\",\"f4\",\"f5\",\"f6\",\"f7\",\"f8\",\"f9\",\"f10\",\"f11\",\"f12\",\"f13\",\"f14\",\"f15\",\"l\"]) #read the data\n",
    "\n",
    "my_data = (my_data - my_data.mean())/my_data.std()\n",
    "\n",
    "#prepare X matrix\n",
    "X = my_data.iloc[:,0:8].values\n",
    "#ones = np.ones([X.shape[0],1])\n",
    "#X = np.concatenate((ones,X),axis=1) #X = pd.DataFrame.from_records(X)\n",
    "##prepare y matrix\n",
    "y = my_data.iloc[:,8:9].values #.values converts it from pandas.core.frame.DataFrame to numpy.ndarray\n",
    "##prepare w matrix\n",
    "w = np.zeros([1,8])\n",
    "\n",
    "b = 0\n",
    "\n",
    "C = 10\n",
    "\n",
    "\n",
    "#cost/loss function\n",
    "def Cost(X,y,w,b,C):\n",
    "    #regular format expression\n",
    "    sum1 = X @ w.T + b\n",
    "    sum1 = y * sum1\n",
    "    sum1 = 1-sum1  \n",
    "    sum1 = sum1.clip(min=0)\n",
    "    sum2 = np.power(w,2)\n",
    "    return (np.sum(sum2)/2) + C*np.sum(sum1)\n",
    "\n",
    "print(\"Cost\",Cost(X,y,w,b,10))\n",
    "\n",
    "def hinge_loss(w,x,y):\n",
    "    \"\"\" evaluates hinge loss and its gradient at w\n",
    "\n",
    "    rows of x are data points\n",
    "    y is a vector of labels\n",
    "    \"\"\"\n",
    "    loss,grad = 0,0\n",
    "    for (x_,y_) in zip(x,y):\n",
    "        v = y_*(np.dot(w,x_)+b)\n",
    "#        print(v)\n",
    "        loss += max(0,1-v)\n",
    "        grad += 0 if v > 1 else -y_*x_\n",
    "    return loss\n",
    "print(\"loss\",hinge_loss(w,X,y))\n",
    "\n",
    "def L_w(X,y,w,b,j):\n",
    "    z = X @ w.T + b\n",
    "    z = y*z\n",
    "    v = np.zeros([760,1])\n",
    "    Xj = X[:,j]\n",
    "    Xj = Xj.reshape(len(X),1)\n",
    "    z = np.where(z>=1,v,-1*y*Xj)\n",
    "    return z\n",
    " \n",
    "def L_b(y,w,b):\n",
    "    z = X @ w.T + b\n",
    "    z = y*z\n",
    "    v = np.zeros([760,1])\n",
    "    z = np.where(z>=1,v,-1*y)\n",
    "    return z\n",
    "\n",
    "def Batch_Gradient_w(X,y,w,b,j):\n",
    "    return w[0,j]+C*np.sum(L_w(X,y,w,b,j))\n",
    "\n",
    "def Batch_Gradient_b(y,w,b):\n",
    "    return b + C*np.sum(L_b(y,w,b))\n",
    "\n",
    "\n",
    "def Stocastic_Gradient_w(X,y,w,b,j,i):\n",
    "    Lw = L_w(X,y,w,b,j)\n",
    "    return w[0,j] + C*Lw[i,0]\n",
    "\n",
    "def Stocastic_Gradient_b(y,w,b,i):\n",
    "    Lb = L_b(y,w,b)\n",
    "    return b + C*Lb[i,0]\n",
    "\n",
    "def Mini_Batch_Gradient_w(X,y,w,b,j,batch_size,l):\n",
    "   i = int(l*batch_size+1)\n",
    "   n = int(min(len(X),((l+1)*batch_size)))\n",
    "   Lw = L_w(X,y,w,b,j)\n",
    "   batch_sum = np.sum(Lw[i:n])\n",
    "#   for k in range(i,n):\n",
    "#       batch_sum += Lw[k,0]\n",
    "#   print(w[0,j]+C*np.sum(Lw[i:n]))\n",
    "   return w[0,j]+C*batch_sum\n",
    "\n",
    "def Mini_Batch_Gradient_b(y,w,b,batch_size,l):\n",
    "    i = int(l*batch_size+1)\n",
    "    n = int(min(len(X),((l+1)*batch_size)))\n",
    "    Lb = L_b(y,w,b)\n",
    "    batch_sum = np.sum(Lb[i:n])\n",
    "    return b + C*batch_sum\n",
    "\n",
    "def dCost(cost,k):\n",
    "    if(k != 0):\n",
    "        return (abs(cost[k-1]-cost[k])*100)/cost[k-1]\n",
    "    else: return 1\n",
    "\n",
    "def Batch_SVM(X,y,w,b,C,alpha,epsilon):\n",
    "    \n",
    "    k=0\n",
    "    cost = []\n",
    "    tempCost = 10\n",
    "    while(tempCost>epsilon):\n",
    "        \n",
    "        for j in range(len(w[0])):\n",
    "          w[0][j] = w[0][j] - alpha*Batch_Gradient_w(X,y,w,b,j)\n",
    "        b = b - alpha*Batch_Gradient_b(y,w,b)\n",
    "        \n",
    "        cost.append(Cost(X,y,w,b,C))\n",
    "        \n",
    "        tempCost = dCost(cost,k)\n",
    "          \n",
    "        k += 1\n",
    "    return w,cost,k,b\n",
    "\n",
    "\n",
    "\n",
    "def Stocastic_SVM(X,y,w,b,C,alpha,epsilon):\n",
    "    \n",
    "    k=0\n",
    "    i=0\n",
    "    cost = []\n",
    "    costk = [10]\n",
    "    tempCost = 10.0\n",
    "    m = len(X)\n",
    "    while(costk[k]>epsilon):\n",
    "        \n",
    "        for j in range(len(w[0])):\n",
    "          w[0][j] = w[0][j] - alpha*Stocastic_Gradient_w(X,y,w,b,j,i)\n",
    "        b = b - alpha*Stocastic_Gradient_b(y,w,b,i)\n",
    "        \n",
    "        \n",
    "        cost.append(Cost(X,y,w,b,C))    \n",
    "        tempCost = dCost(cost,k)\n",
    "        costk.append(0.5*costk[k-1]+0.5*tempCost)\n",
    "        \n",
    "        \n",
    "        i = (i+1)%m\n",
    "        k += 1\n",
    "    return w,cost,k,b\n",
    "\n",
    "def Mini_Batch_SVM(X,y,w,b,C,alpha,epsilon):\n",
    "    \n",
    "    l = 0\n",
    "    k = 0\n",
    "    batch_size = 4\n",
    "    cost = []\n",
    "    costk = [10]\n",
    "    \n",
    "    while(costk[k]>epsilon):\n",
    "        for j in range (len(w[0])):\n",
    "            w[0][j] = w[0][j] - alpha*Mini_Batch_Gradient_w(X,y,w,b,j,batch_size,l)\n",
    "        b = b - alpha*Mini_Batch_Gradient_b(y,w,b,batch_size,l)\n",
    "        \n",
    "        cost.append(Cost(X,y,w,b,C))    \n",
    "        tempCost = dCost(cost,k)\n",
    "        costk.append(0.5*costk[k-1]+0.5*tempCost)\n",
    "            \n",
    "        l = (l+1) % ((len(X)+batch_size-1)/batch_size)\n",
    "        k = k+1\n",
    "        \n",
    "    return w,cost,k,b \n",
    "   \n",
    "\n",
    "\n",
    "w = np.zeros([1,8])\n",
    "b = 0\n",
    "alpha = 0.00001\n",
    "epsilon = 0.004\n",
    "w_,cost1,count1,b_ = Batch_SVM(X,y,w,b,C,alpha,epsilon)\n",
    "print(Cost(X,y,w_,b_,C))\n",
    "\n",
    "w = np.zeros([1,8])\n",
    "b = 0\n",
    "alpha = 0.0001\n",
    "epsilon = 0.001\n",
    "w_,cost2,count2,b_ = Stocastic_SVM(X,y,w,b,C,alpha,epsilon)\n",
    "print(Cost(X,y,w_,b_,C))\n",
    "\n",
    "w = np.zeros([1,8])\n",
    "b = 0\n",
    "alpha = 0.0001\n",
    "epsilon = 0.0016\n",
    "w_,cost3,count3,b_ = Mini_Batch_SVM(X,y,w,b,C,alpha,epsilon)\n",
    "print(Cost(X,y,w_,b_,C))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#plot the cost\n",
    "fig, ax = plt.subplots()  \n",
    "ax.plot(np.arange(count1), cost1, 'r', label = 'Batch')\n",
    "ax.plot(np.arange(count2), cost2, 'b', label = 'Stocastic')\n",
    "ax.plot(np.arange(count3), cost3, 'g', label = 'Mini Batch')\n",
    "    \n",
    "ax.set_xlabel('Iterations')  \n",
    "ax.set_ylabel('Cost')  \n",
    "ax.set_title('Loss function over iterations') \n",
    "\n",
    "\n",
    "#print(Gradient(X,y,w,b,0))\n",
    "###\n",
    "##print the loss function value before training\n",
    "#initialCost = Cost(X,y,theta,lambd)\n",
    "#print(\"Cost function before training\",initialCost)\n",
    "#\n",
    "##gradient function Lasso Regularization (partial derivative with respect to thetaj)\n",
    "#def Gradient(X,y,theta,lambd,j):\n",
    "#  m = len(X)\n",
    "#  Xj = X[:,j]\n",
    "#  Xj = Xj.reshape(len(X),1)\n",
    "#  sum = ((X @ theta.T)-y)*Xj\n",
    "#  if(theta[0][j]==0):\n",
    "#    lasso = 1\n",
    "#  else:\n",
    "#    lasso = (lambd*theta[0][j])/(2*m*abs(theta[0][j]))\n",
    "#  \n",
    "#  return (np.sum(sum)/m)+lasso\n",
    "##linear Regression with Quadratic Regularization\n",
    "#def LinearRegression(X,y,theta,alpha,epsilon,lambd):\n",
    "#    cost = []\n",
    "#    k = 0\n",
    "#    tempCost = 10\n",
    "#    while(tempCost>epsilon):\n",
    "#        \n",
    "#        for j in range(len(theta[0])):\n",
    "#          theta[0][j] = theta[0][j] - alpha*Gradient(X,y,theta,lambd,j)\n",
    "#        \n",
    "#        cost.append(Cost(X, y, theta, lambd))\n",
    "#        \n",
    "#        if(k != 0):\n",
    "#          tempCost = (abs(cost[k-1]-cost[k])*100)/cost[k-1]\n",
    "#          \n",
    "#        k += 1\n",
    "#    \n",
    "#    return theta,cost, k\n",
    "#  \n",
    "##running the gd and cost function\n",
    "#g,cost, count = LinearRegression(X,y,theta,alpha,epsilon,lambd)\n",
    "#\n",
    "##print loss/cost function after training\n",
    "#finalCost = Cost(X,y,g,lambd)\n",
    "#print(\"Cost function after training\",finalCost)\n",
    "#\n",
    "#\n",
    "#def TestingCost(X,y,theta):\n",
    "#  \n",
    "#    sum1 = np.power(((X @ theta.T)-y),2)\n",
    "#    \n",
    "#    return np.sum(sum1)/(2 * len(X))\n",
    "#\n",
    "#test_data = pd.read_csv('raw_testing_data.txt',names=[\"f1\",\"f2\",\"f3\",\"f4\",\"f5\",\"f6\",\"f7\",\"f8\",\"f9\",\"f10\",\"f11\",\"f12\",\"f13\",\"f14\",\"f15\",\"l\"]) #read the data\n",
    "#test_data = (test_data - test_data.mean())/test_data.std()\n",
    "#\n",
    "#X = test_data.iloc[:,0:15]\n",
    "#\n",
    "#ones = np.ones([X.shape[0],1])\n",
    "#X = np.concatenate((ones,X),axis=1)\n",
    "#\n",
    "#y = test_data.iloc[:,15:16].values #.values converts it from pandas.core.frame.DataFrame to numpy.ndarray\n",
    "#theta = np.zeros([1,16])\n",
    "#\n",
    "#\n",
    "#print(\"Cost function of testing data\",TestingCost(X,y,g))\n",
    "#\n",
    "#def Rounding(theta):\n",
    "#  for i in range(len(theta[0])):\n",
    "#      if(abs(g[0][i])<0.005): theta[0][i] = 0\n",
    "#\n",
    "##print trained parameters of theta\n",
    "#print(g[0])\n",
    "#Rounding(g)\n",
    "#print(g[0])\n",
    "##plot the cost\n",
    "#fig, ax = plt.subplots()  \n",
    "#ax.plot(np.arange(count), cost, 'r')  \n",
    "#ax.set_xlabel('Iterations')  \n",
    "#ax.set_ylabel('Cost')  \n",
    "#ax.set_title('Loss function over iterations') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
