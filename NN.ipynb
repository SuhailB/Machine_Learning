{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ad hoc mnist instances\n",
    "#from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "data = pd.read_csv('train.csv').values\n",
    "X = data[:,1:]\n",
    "Y = data[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(X, Y, folds, normalize):\n",
    "    \n",
    "    if(normalize==True): X = X/X.max()\n",
    "    \n",
    "    kf = KFold(n_splits=folds)\n",
    "    kf.get_n_splits(X)\n",
    "    \n",
    "    print(kf)  \n",
    "    train_data = []\n",
    "    test_data = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "        train_data.append((X_train,Y_train))\n",
    "        test_data.append((X_test, Y_test))\n",
    "        \n",
    "    return train_data, test_data\n",
    "\n",
    "def getTrainingData(X, Y, train_ratio, normalize=True):\n",
    "    \n",
    "    TRAINING_SIZE = (int)(len(data)*train_ratio)\n",
    "    \n",
    "    if(normalize==True): X = X/X.max()\n",
    "    \n",
    "    X_train = X[0:TRAINING_SIZE,:]\n",
    "    Y_train = Y[0:TRAINING_SIZE]\n",
    "   \n",
    "    return X_train, Y_train\n",
    "\n",
    "def getTestingData(X, Y, train_ratio, test_size, test_remaining=False, normalize=True):\n",
    "    \n",
    "    TRAINING_SIZE = (int)(len(data)*train_ratio)\n",
    "    \n",
    "    if(normalize==True): X = X/X.max()\n",
    "    \n",
    "    if test_remaining==True:\n",
    "        X_test = X[TRAINING_SIZE:,:]\n",
    "        Y_test = Y[TRAINING_SIZE:]\n",
    "    else:\n",
    "        X_test = X[TRAINING_SIZE:TRAINING_SIZE+test_size,:]\n",
    "        Y_test = Y[TRAINING_SIZE:TRAINING_SIZE+test_size]\n",
    "    \n",
    "    return X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=5, random_state=None, shuffle=False)\n",
      "KFold(n_splits=5, random_state=None, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "set_names = [\"Non-normalized Data\", \"Normalized Data\"]\n",
    "rawTrainSets, rawTestSets = getData(X, Y, 5, False)\n",
    "normTrainSets, normTestSets = getData(X, Y, 5, True)\n",
    "train_sets = [rawTrainSets, normTrainSets]\n",
    "test_sets = [rawTestSets, normTestSets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [{'activation':'identity', 'solver':'lbfgs'},\n",
    "          {'activation':'identity', 'solver':'sgd'},\n",
    "          {'activation':'identity', 'solver':'adam'},\n",
    "          {'activation':'logistic', 'solver':'lbfgs'},\n",
    "          {'activation':'logistic', 'solver':'sgd'},\n",
    "          {'activation':'logistic', 'solver':'adam'},\n",
    "          {'activation':'tanh', 'solver':'lbfgs'},\n",
    "          {'activation':'tanh', 'solver':'sgd'},\n",
    "          {'activation':'tanh', 'solver':'adam'},\n",
    "          {'activation':'relu', 'solver':'lbfgs'},\n",
    "          {'activation':'relu', 'solver':'sgd'},\n",
    "          {'activation':'relu', 'solver':'adam'}]\n",
    "plot_args = [{'c': 'red', 'linestyle': ''},\n",
    "             {'c': 'red', 'linestyle': '-'},\n",
    "             {'c': 'red', 'linestyle': '--'},\n",
    "             {'c': 'green', 'linestyle': ''},\n",
    "             {'c': 'green', 'linestyle': '-'},\n",
    "             {'c': 'green', 'linestyle': '--'},\n",
    "             {'c': 'blue', 'linestyle': ''},\n",
    "             {'c': 'blue', 'linestyle': '-'},\n",
    "             {'c': 'blue', 'linestyle': '--'},\n",
    "             {'c': 'purple', 'linestyle': ''},\n",
    "             {'c': 'purple', 'linestyle': '-'},\n",
    "             {'c': 'purple', 'linestyle': '--'}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models using Non-normalized Data \n",
      "Training using:  {'activation': 'identity', 'solver': 'lbfgs'}\n",
      "209.9855091571808\n",
      "Done training!\n",
      "Training using:  {'activation': 'identity', 'solver': 'sgd'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Programs\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_base.py:91: RuntimeWarning: overflow encountered in subtract\n",
      "  tmp = X - X.max(axis=1)[:, np.newaxis]\n",
      "E:\\Programs\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_base.py:91: RuntimeWarning: invalid value encountered in subtract\n",
      "  tmp = X - X.max(axis=1)[:, np.newaxis]\n",
      "E:\\Programs\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315.9089150428772\n",
      "Done training!\n",
      "Training using:  {'activation': 'identity', 'solver': 'adam'}\n",
      "150.68810868263245\n",
      "Done training!\n",
      "Training using:  {'activation': 'logistic', 'solver': 'lbfgs'}\n",
      "257.8545506000519\n",
      "Done training!\n",
      "Training using:  {'activation': 'logistic', 'solver': 'sgd'}\n",
      "280.8732399940491\n",
      "Done training!\n",
      "Training using:  {'activation': 'logistic', 'solver': 'adam'}\n",
      "79.35916996002197\n",
      "Done training!\n",
      "Training using:  {'activation': 'tanh', 'solver': 'lbfgs'}\n",
      "238.7781949043274\n",
      "Done training!\n",
      "Training using:  {'activation': 'tanh', 'solver': 'sgd'}\n",
      "320.1553990840912\n",
      "Done training!\n",
      "Training using:  {'activation': 'tanh', 'solver': 'adam'}\n",
      "136.13372564315796\n",
      "Done training!\n",
      "Training using:  {'activation': 'relu', 'solver': 'lbfgs'}\n",
      "217.15334558486938\n",
      "Done training!\n",
      "Training using:  {'activation': 'relu', 'solver': 'sgd'}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "mlps = []\n",
    "ex_times = []\n",
    "for train_set, set_name in zip(train_sets, set_names):\n",
    "    X_train, y_train = train_set[4]\n",
    "    print(\"Training models using %s \" % set_name)\n",
    "    for param, plot_arg in zip(params, plot_args):\n",
    "        print(\"Training using: \", param)\n",
    "        mlp = MLPClassifier(**param, max_iter = 300)\n",
    "        start_t = time.time()\n",
    "        mlp.fit(X_train, y_train)\n",
    "        end_t = time.time()\n",
    "        ex_times.append(end_t-start_t)\n",
    "        print(end_t-start_t)\n",
    "        mlps.append(mlp)\n",
    "        print(\"Done training!\")\n",
    "print(\"Done all training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# i=0\n",
    "\n",
    "\n",
    "# # for test_set, set_name in zip(test_sets, set_names):\n",
    "# #     X_test, y_test = test_set[4]\n",
    "# #     print(\"Testing models using %s \" % set_name)\n",
    "# #     j=12*i\n",
    "# #     f = 12*(i+1)\n",
    "# #     while(j<f):\n",
    "# #         str1 = str(params[j%12])\n",
    "# #         str2 = str(mlps[j].score(X_test, y_test))\n",
    "# #         print(\"%-60s \\t %s\\n\" % (str1, str2), end='')\n",
    "# #         j += 1\n",
    "# #     i += 1\n",
    "\n",
    "# fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "# for test_set, set_name in zip(test_sets, set_names):\n",
    "#     X_test, y_test = test_set[4]\n",
    "#     j=12*i\n",
    "#     f = 12*(i+1)\n",
    "#     while(j<f):\n",
    "#         str1 = str(params[j%12])\n",
    "#         str2 = str(mlps[j].score(X_test, y_test))\n",
    "#         print(\"%-60s \\t %s\\n\" % (str1, str2), end='')\n",
    "#         print(\"Training set loss: %f\" % mlp.loss_)\n",
    "#         print(mlp.loss_curve_)\n",
    "#         axes.plot(mlp.loss_curve_, label=str(params[j%12]), **plot_arg)\n",
    "#         j += 1\n",
    "#     i += 1\n",
    "    \n",
    "# fig.legend(axes.get_lines(), labels, ncol = 2, loc=\"upper center\")\n",
    "# plt.show()\n",
    "\n",
    "test_sets = [rawTestSets, normTestSets]\n",
    "# i=0\n",
    "# for test_set, set_name in zip(test_sets, set_names):\n",
    "#     print(\"Testing models using %s \" % set_name)\n",
    "#     j=5*i\n",
    "#     f = 5*(i+1)\n",
    "#     while(j<f):\n",
    "#         X_test, y_test = test_set[j%5]\n",
    "#         str1 = \"fold# \" + str(j%5)\n",
    "#         str2 = str(mlps_cv[j].score(X_test, y_test))\n",
    "#         print(\"%s \\t %s\\n\" % (str1, str2), end='')\n",
    "#         j += 1\n",
    "#     i += 1\n",
    "\n",
    "i=0\n",
    "\n",
    "test_sets = [rawTestSets, normTestSets]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(8, 1, figsize=(20, 60))\n",
    "# print(len(axes[0]))\n",
    "count = 0\n",
    "for test_set, set_name in zip(test_sets, set_names):\n",
    "\n",
    "    X_test, y_test = test_set[4]\n",
    "    j=12*i\n",
    "    f = 12*(i+1)\n",
    "    while(j<f):\n",
    "        str1 = str(params[j%12])\n",
    "        str2 = str(mlps[j].score(X_test, y_test))\n",
    "        print(\"%-60s \\t %s\\n\" % (str1, str2), end='')\n",
    "        print(\"Training set loss: %f\" % mlps[j].loss_)\n",
    "        j += 1\n",
    "    k=4*i\n",
    "    f = 4*(i+1)\n",
    "    while(k<f):\n",
    "#         print(k)\n",
    "        s = k*3\n",
    "        for l in range(s, s+3):\n",
    "            \n",
    "            activation = params[l%12].get(\"activation\")\n",
    "            solver = params[l%12].get(\"solver\")\n",
    "            if(solver!=\"lbfgs\"):\n",
    "                print(k)\n",
    "                axes[k].plot(mlps[l].loss_curve_, **plot_args[l%12])\n",
    "                count +=1\n",
    "        k += 1\n",
    "        \n",
    "    i += 1\n",
    "# print(count)  \n",
    "    \n",
    "# fig.legend(axes[0].get_lines(), ncol=4, loc=\"upper center\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlps_cv = []\n",
    "for train_set, set_name in zip(train_sets, set_names):\n",
    "    print(\"Training models using %s \" % set_name)\n",
    "    for cv_train_set in train_set:\n",
    "        X_train, y_train = cv_train_set\n",
    "        mlp = MLPClassifier()\n",
    "        mlp.fit(X_train, y_train)\n",
    "        mlps_cv.append(mlp)\n",
    "        print(\"Done training!\")\n",
    "print(\"Done all training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sets = [rawTestSets, normTestSets]\n",
    "i=0\n",
    "for test_set, set_name in zip(test_sets, set_names):\n",
    "    print(\"Testing models using %s \" % set_name)\n",
    "    j=5*i\n",
    "    f = 5*(i+1)\n",
    "    while(j<f):\n",
    "        X_test, y_test = test_set[j%5]\n",
    "        str1 = \"fold# \" + str(j%5)\n",
    "        str2 = str(mlps_cv[j].score(X_test, y_test))\n",
    "        print(\"%s \\t %s\\n\" % (str1, str2), end='')\n",
    "        j += 1\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writePredictions(clf, X):\n",
    "    IDs = np.arange(start=1,stop=len(X)+1)\n",
    "    IDs= IDs.reshape(len(X),1)\n",
    "    predictions = clf.predict(X)\n",
    "    predictions = predictions.reshape(len(X),1)\n",
    "    output = np.concatenate((IDs,predictions), axis=1)\n",
    "    np.savetxt(\"sample_submissionNN.csv\", output, delimiter=\",\", fmt = '%d', header = \"ImageID,Label\", comments='')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writePredictions(mlps_cv[5],test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Programs\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5568829923542506\n",
      "1.8156928122635128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Programs\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#hidden_layer_sizes=(80),\n",
    "X_train, y_train = train_sets[1][4]\n",
    "X_test, y_test = test_sets[1][4]\n",
    "mlp1 = MLPClassifier(**{'activation':'relu', 'solver':'adam'},random_state = 0, max_iter =1)\n",
    "mlp2 = MLPClassifier(**{'activation':'relu', 'solver':'sgd'},random_state = 0,max_iter =1)\n",
    "mlp2.fit(X_train, y_train)\n",
    "mlp1.fit(X_train, y_train)\n",
    "print(mlp1.loss_)\n",
    "print(mlp2.loss_)\n",
    "\n",
    "\n",
    "# print(mlp1.score(X_test, y_test))\n",
    "# print(mlp2.score(X_test, y_test))\n",
    "# print(mlp1.loss_curve_)\n",
    "# print(mlp2.loss_curve_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9778571428571429"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.9778571428571429"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
